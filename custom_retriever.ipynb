{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8873c5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vineetdorikar/Developer/projects/RAG_blog_series/rag_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "#initializing embedding mdoel\n",
    "embed_model = HuggingFaceEmbedding(model_name='BAAI/bge-small-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09855918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "#initializing llm\n",
    "llm = Ollama(model=\"llama2\", request_timeout=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e65d161",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "connection to server at \"localhost\" (::1), port 5432 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m port = \u001b[33m'\u001b[39m\u001b[33m5432\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#connecting to existing database named 'postgres'\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m db = \u001b[43mpsycopg2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpostgres\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mport\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# autocommit = true, cause create database and drop database commands \u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# can not be executed in transaction state.\u001b[39;00m\n\u001b[32m     21\u001b[39m db.autocommit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/projects/RAG_blog_series/rag_venv/lib/python3.12/site-packages/psycopg2/__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m     conn.cursor_factory = cursor_factory\n",
      "\u001b[31mOperationalError\u001b[39m: connection to server at \"localhost\" (::1), port 5432 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "#important parameters used for database connections\n",
    "db_name = 'vector_store'\n",
    "host = 'localhost'\n",
    "user = 'vineet'\n",
    "password = 'password'\n",
    "port = '5432'\n",
    "\n",
    "#connecting to existing database named 'postgres'\n",
    "db = psycopg2.connect(\n",
    "    host=host,\n",
    "    database=\"postgres\",\n",
    "    user=user,\n",
    "    password=password,\n",
    "    port=port\n",
    ")\n",
    "\n",
    "# autocommit = true, cause create database and drop database commands \n",
    "# can not be executed in transaction state.\n",
    "db.autocommit = True\n",
    "\n",
    "#removing old database with same name and creating new database\n",
    "with db.cursor() as c:\n",
    "    c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "    c.execute(f\"CREATE DATABASE {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "# creating vector database from our norimal database by using PGVectorStore\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=host,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    table_name=\"research paper\",\n",
    "    embed_dim=384\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca42ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "# loading pdf's in our environment \n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "\n",
    "documents = loader.load(file_path=\"./data/CLIP.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10139a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# chunking or splitting our docs in smaller chunks\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "\n",
    "chunks = []\n",
    "chunk_idx = []\n",
    "\n",
    "for id, doc in enumerate(documents):\n",
    "    cur_chunks = splitter.split_text(doc.text)\n",
    "    chunks.extend(cur_chunks)\n",
    "    chunk_idx.extend([id]* len(cur_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ce1794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "# saving those chunks as nodes \n",
    "\n",
    "nodes = []\n",
    "\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    node = TextNode(text=chunk)\n",
    "    src_doc = documents[chunk_idx[idx]]\n",
    "    node.metadata = src_doc.metadata\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fe7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding part\n",
    "\n",
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(node.get_content(metadata_mode=all))\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4af2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_pages': 48, 'file_path': './data/CLIP.pdf', 'source': '4'}\n"
     ]
    }
   ],
   "source": [
    "print(nodes[6].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315e500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3238681a-c33a-40a7-8c7e-185583610c93',\n",
       " '8831820e-0ceb-4b13-be22-780d2573509a',\n",
       " '2aa821b4-4552-4fee-952c-cb03977d6c06',\n",
       " '0d1b5714-21a9-464c-b7df-9017f4d7d57f',\n",
       " '769851e3-5e46-49a3-8144-ebd05157b348',\n",
       " 'f1d9f14a-a9c1-46e5-b8ea-e56484934870',\n",
       " '4b075cf3-9913-4176-a282-4f6757777c01',\n",
       " 'e95191f3-6b82-45af-a72b-d7f2057ca47a',\n",
       " 'cebc0f8c-3d6f-418b-a5f7-bb2cb830b2b2',\n",
       " 'aa3c02f0-7dc3-4a2a-b383-0f202d522bb2',\n",
       " '70570ca4-5ab4-4680-9775-4e1443731f3c',\n",
       " '1d1907de-9419-4859-b41d-ab0a6016a3e8',\n",
       " '840543a2-30e1-4880-982b-46f04a3f7cc6',\n",
       " '34a2ed76-5ae1-4652-81e0-d4752c267623',\n",
       " '5d0b181d-d640-4ec0-8b3a-18de2c212c4b',\n",
       " '91168979-6dca-466f-ae09-65958970fb8b',\n",
       " '308684e3-0228-40e2-bc2a-a5953a9f5cd9',\n",
       " 'ddb91a14-9455-4e29-8953-75296f6917fe',\n",
       " '2e5d4cd3-392b-4a08-8a6c-7b5e5db2f6b5',\n",
       " '8f62395a-a3c0-47e4-9689-e97b6ac8e8fe',\n",
       " 'dc97a4d5-42a9-45c5-8b45-172d3a4e8df8',\n",
       " '935d8e3f-c6b6-4fe5-a4ad-553f9e42ba37',\n",
       " '49e06095-3476-42d9-8828-101991706240',\n",
       " '204166af-c580-4053-9cc9-e5a510cce8fd',\n",
       " '4ddb1448-b054-4cce-9f7c-0547a7dc179e',\n",
       " 'e37e6c96-efe6-4e90-9451-f353c57a86f6',\n",
       " '2cb79dd8-523a-426a-8e68-686d80e1843b',\n",
       " 'd568a525-e7ce-416a-859d-851c8258c821',\n",
       " '825bcc1e-5c04-4fac-aad9-f51fc908ab9c',\n",
       " 'c68528bf-f67d-4255-8245-d9cff887a7bb',\n",
       " '714a452b-722e-4e14-a481-e44cf118ceb9',\n",
       " '622b4f79-ba6b-4fb2-846e-771084a8f694',\n",
       " '80f7afce-8d55-4a09-9117-621fee0573bc',\n",
       " '6b6fef12-ceb3-4890-ad36-b77f8bc8afac',\n",
       " '7c080ed4-c8db-4742-a903-349bdd13af66',\n",
       " 'eb93d9b2-5fb0-41c3-859e-298f5c755122',\n",
       " 'bacdf2cf-d36f-439a-83bc-454151be5a4b',\n",
       " 'e4e23a92-ca40-498a-aae7-78a043981020',\n",
       " 'aa4e675d-c3a3-4629-bd5b-89060d929659',\n",
       " '987cbf79-a0c5-46f1-9b31-a4e56e0b286c',\n",
       " 'a36f69da-e54e-4adb-8647-3c35f94a2245',\n",
       " '06646125-f4ac-48df-890f-a1d3685e96b9',\n",
       " '6d736321-39b5-4592-a8bf-1670f2f9ba56',\n",
       " 'b35fa693-ff0d-444e-8cc7-3ea7fe06b7e4',\n",
       " '6e713448-b59a-406c-bc35-acb077df3604',\n",
       " '2ef87bc8-6596-4428-93b9-56e41d460c8d',\n",
       " 'f0188de0-e017-472a-8d5a-bb54971d5235',\n",
       " '0f19aeb8-d530-479f-9f05-1ff04233e41b',\n",
       " 'd55915de-e96e-44e3-b129-f36b41c80706',\n",
       " '7f08e3b8-c8ab-4679-9f29-76a669f3f3ba',\n",
       " 'a9b803d2-24f2-4e77-8934-fc330a211908',\n",
       " '6a81862b-1fd0-42e0-89a3-a9451809c5e5',\n",
       " 'bf7f4449-8231-415f-9562-07b30bca603b',\n",
       " '279ddd1d-1c08-4a15-9668-42158ef1da6e',\n",
       " '39ee8264-353a-4b19-ae18-ff031b1bd5d7',\n",
       " '5ee27f1d-b387-420c-8e3b-edc406b85020',\n",
       " 'cb6b75e0-f118-4f3e-bfd2-9dfd0284ca88',\n",
       " '310302a6-51f2-40e5-a813-8c8f0095950c',\n",
       " '9be2d181-1f55-47ac-b6a9-7be7c6e8d2fa',\n",
       " 'cfc0ff6f-6b83-430c-a1c5-e0d0de1ae726',\n",
       " 'f5fca6a9-b92f-4ed8-b1cb-5fe40726b053',\n",
       " 'b199285c-01f2-4530-8d2d-68f1f247fda9',\n",
       " 'a2284131-1fe9-4741-9e61-c1e1fb88b112',\n",
       " '5251033e-b21d-4915-be02-c20936812ed1',\n",
       " '2450f7ce-c4de-49f9-9f28-47bf1796f77e',\n",
       " 'c8ad356d-4321-4e54-957b-017d799fe004',\n",
       " 'd8d72392-a00d-4d6c-8c20-145497416005',\n",
       " '429b2324-b4cc-421b-9cd2-080d7eacbdef',\n",
       " 'be28d49e-1fbd-4a7e-bfee-2b89b5ac3254',\n",
       " 'a8b70d17-fc6f-4eeb-9eb1-cdf6e20f6a83',\n",
       " '2e247200-7957-4bb3-ab31-62c032eb5742',\n",
       " '8b330882-7f10-41e8-860e-2c2a09b72f32',\n",
       " 'e3e84ff9-8b6e-4e78-b406-395da092ce0d',\n",
       " 'b298582e-0a5f-4494-ad75-140052d06ef7',\n",
       " '5e2f946b-6149-4cb4-afb8-9c3653ddb96c',\n",
       " 'd32cd806-4f05-4546-9e7f-ce6e73d3703c',\n",
       " '45e89440-c914-4482-b2ae-9dd81a13f922',\n",
       " 'b942a80f-bb1c-4926-9160-f2654f0afd7f',\n",
       " '3397899d-29f5-4074-a647-cc04003a0707',\n",
       " 'ffa61398-c2eb-4c73-bc99-658d8ee63af1',\n",
       " '1dd58ffb-c513-4bb2-b8d8-0e59c95788e0',\n",
       " '01a964ae-f18b-496b-b4d3-18584fdf87d0',\n",
       " 'c88d34f7-a4e5-41a3-95c6-ce564defb136',\n",
       " '9c83b5e0-0867-4c16-9c61-634c60ff5587',\n",
       " '08745a05-56de-48c8-9835-f84dff99f5f5',\n",
       " 'f61e84f1-37a9-439e-9575-76cb0baa55c0',\n",
       " '49318cb3-28db-4feb-84e3-47da1c76e6d0',\n",
       " '98c88e9e-8205-4750-84d1-04d976ca307c',\n",
       " '7371c5e2-18d1-43ad-871b-6caef9efa04a',\n",
       " '41ef8dea-050b-4cac-b351-65bc959ff0ff',\n",
       " '587df6ad-b140-4994-8fd5-fd465b35c629',\n",
       " '3fbd7cc1-74a8-46b9-a37e-1309735fa9ab',\n",
       " 'defba5b3-7a1c-4d05-b8e3-81acd653ee4d',\n",
       " 'd095f516-f37c-491b-8147-b9f19fe73615',\n",
       " '2dfd768e-74c1-4859-9517-889170f5178a',\n",
       " '6d981a03-516b-46ee-8900-be6f2a87d5dd',\n",
       " '964aa113-3640-4e52-90d9-8dd2aef8f68f',\n",
       " 'ef7e16d0-5729-4173-830f-0529fe5bc0e7']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding all nodes to our vector database \n",
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db13d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"do we use image embedding in CLIP?\"\n",
    "\n",
    "# embedding our query \n",
    "query_embedding = embed_model.get_query_embedding(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b64f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "# creating query object \n",
    "query_obj = VectorStoreQuery(query_embedding=query_embedding, similarity_top_k=2, mode=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26f9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = vector_store.query(query_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc9c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Transferable Visual Models From Natural Language Supervision\n",
      "20\n",
      "CLIP also does not address the poor data efﬁciency of deep\n",
      "learning. Instead CLIP compensates by using a source of\n",
      "supervision that can be scaled to hundreds of millions of\n",
      "training examples. If every image seen during training of\n",
      "a CLIP model was presented at a rate of one per second,\n",
      "it would take 405 years to iterate through the 12.8 billion\n",
      "images seen over 32 training epochs. Combining CLIP\n",
      "with self-supervision (Henaff, 2020; Chen et al., 2020c) and\n",
      "self-training (Lee; Xie et al., 2020) methods is a promising\n",
      "direction given their demonstrated ability to improve data\n",
      "efﬁciency over standard supervised learning.\n",
      "Our methodology has several signiﬁcant limitations. De-\n",
      "spite our focus on zero-shot transfer, we repeatedly queried\n",
      "performance on full validation sets to guide the develop-\n",
      "ment of CLIP. These validation sets often have thousands\n",
      "of examples, which is unrealistic for true zero-shot sce-\n",
      "narios. Similar concerns have been raised in the ﬁeld of\n",
      "semi-supervised learning (Oliver et al., 2018). Another po-\n",
      "tential issue is our selection of evaluation datasets. While\n",
      "we have reported results on Kornblith et al. (2019)’s 12\n",
      "dataset evaluation suite as a standardized collection, our\n",
      "main results use a somewhat haphazardly assembled col-\n",
      "lection of 27 datasets that is undeniably co-adapted with\n",
      "the development and capabilities of CLIP. Creating a new\n",
      "benchmark of tasks designed explicitly to evaluate broad\n",
      "zero-shot transfer capabilities, rather than re-using existing\n",
      "supervised datasets, would help address these issues.\n",
      "CLIP is trained on text paired with images on the internet.\n",
      "These image-text pairs are unﬁltered and uncurated and\n",
      "result in CLIP models learning many social biases. This\n",
      "has been previously demonstrated for image caption models\n",
      "(Bhargava & Forsyth, 2019). We refer readers to Section 7\n",
      "for detailed analysis and quantiﬁcation of these behaviors for\n",
      "CLIP as well as discussion of potential mitigation strategies.\n",
      "While we have emphasized throughout this work that speci-\n",
      "fying image classiﬁers through natural language is a ﬂexible\n",
      "and general interface, it has its own limitations. Many com-\n",
      "plex tasks and visual concepts can be difﬁcult to specify\n",
      "just through text. Actual training examples are undeniably\n",
      "useful but CLIP does not optimize for few-shot performance\n",
      "directly. In our work, we fall back to ﬁtting linear classiﬁers\n",
      "on top of CLIP’s features. This results in a counter-intuitive\n",
      "drop in performance when transitioning from a zero-shot\n",
      "to a few-shot setting. As discussed in Section 4, this is\n",
      "notably different from human performance which shows a\n",
      "large increase from a zero to a one shot setting. Future work\n",
      "is needed to develop methods that combine CLIP’s strong\n",
      "zero-shot performance with efﬁcient few-shot learning.\n",
      "7. Broader Impacts\n",
      "CLIP has a wide range of capabilities due to its ability to\n",
      "carry out arbitrary image classiﬁcation tasks. One can give\n",
      "it images of cats and dogs and ask it to classify cats, or give\n",
      "it images taken in a department store and ask it to classify\n",
      "shoplifters–a task with signiﬁcant social implications and\n",
      "for which AI may be unﬁt. Like any image classiﬁcation\n",
      "system, CLIP’s performance and ﬁtness for purpose need to\n",
      "be evaluated, and its broader impacts analyzed in context.\n",
      "CLIP also introduces a capability that will magnify and alter\n",
      "such issues: CLIP makes it possible to easily create your\n",
      "own classes for categorization (to ‘roll your own classiﬁer’)\n",
      "without a need for re-training. This capability introduces\n",
      "challenges similar to those found in characterizing other,\n",
      "large-scale generative models like GPT-3 (Brown et al.,\n",
      "2020); models that exhibit non-trivial zero-shot (or few-\n",
      "shot) generalization can have a vast range of capabilities,\n",
      "many of which are made clear only after testing for them.\n",
      "Our studies of CLIP in a zero-shot setting show that the\n",
      "model displays signiﬁcant promise for widely-applicable\n",
      "tasks like image retrieval or search. For example, it can ﬁnd\n",
      "relevant images in a database given text, or relevant text\n",
      "given an image.\n"
     ]
    }
   ],
   "source": [
    "print(query_result.nodes[0].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "\n",
    "# saving our nodes and scores in single list \n",
    "\n",
    "nodes_with_scores = []\n",
    "\n",
    "for index,node in enumerate(query_result.nodes):\n",
    "    score : Optional [float] = None\n",
    "    if query_result.similarities is not None:\n",
    "        score = query_result.similarities[index]\n",
    "\n",
    "    nodes_with_scores.append(NodeWithScore(node=node,score=score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66ae33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8907523611029847"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "query_result.similarities[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4e60a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# building custom retriver by inheriting BaseRetriever class\n",
    "\n",
    "class Retriever(BaseRetriever):\n",
    "    def __init__(self,\n",
    "                 vector_store : PGVectorStore,\n",
    "                 embed_model: any,\n",
    "                 query_mode: str = \"default\",\n",
    "                 similarity_top_k: int = 2) -> None:\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        query_embedding = embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_query_obj = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode\n",
    "        )\n",
    "        query_result = vector_store.query(vector_query_obj)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for idx, node in enumerate(query_result.nodes):\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[idx]\n",
    "\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507d0cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever(vector_store, embed_model, similarity_top_k=2, query_mode=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever=retriever, llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e013b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, CLIP does not directly use image embeddings. Instead, it relies on text-image pairs as supervision to train a visual model that can carry out arbitrary image classification tasks. The text-image pairs are unfiltered and uncurated, which can result in the model learning social biases present in the training data. To address this limitation, CLIP falls back to fitting linear classifiers on top of its features when transitioning from a zero-shot to a few-shot setting, which results in a drop in performance. Future work is needed to develop methods that combine CLIP's strong zero-shot performance with efficient few-shot learning.\n"
     ]
    }
   ],
   "source": [
    "query_str = \"do we use image embedding in CLIP?\"\n",
    "\n",
    "query_response = query_engine.query(query_str)\n",
    "\n",
    "print(str(query_response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
